{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-11T12:55:00.755433Z","iopub.status.busy":"2024-04-11T12:55:00.755086Z","iopub.status.idle":"2024-04-11T12:55:17.795468Z","shell.execute_reply":"2024-04-11T12:55:17.794266Z","shell.execute_reply.started":"2024-04-11T12:55:00.755404Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# Load the basic libraries\n","import matplotlib.pyplot as plt\n","import cv2\n","import os\n","import itertools\n","import tensorflow as tf\n","#import skimage\n","import keras\n","#import pydot\n","import seaborn as sns\n","import plotly.express as px\n","\n","# Load data preprocessing's libraries\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n","from tensorflow.keras import utils\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","\n","# Load libraries to build CNN model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import Conv2D,MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization,GlobalMaxPool2D,Activation,GlobalAveragePooling2D\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import Model\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n","\n","# Load VGGFace architecture\n","from keras import backend as K\n","from tensorflow.keras.applications import VGG16\n","from keras.models import load_model\n","from tensorflow.keras.models import Model\n","\n","from tensorflow.keras.applications import EfficientNetB0\n","\n","#load callbacks' libraries\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n","\n","#load visualizing libraries\n","from matplotlib.image import imread\n","from tensorflow.keras.utils import plot_model\n","from skimage.transform import rescale, resize\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.metrics import accuracy_score\n","\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Data Exploration"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T12:55:17.798474Z","iopub.status.busy":"2024-04-11T12:55:17.797598Z","iopub.status.idle":"2024-04-11T12:55:17.804978Z","shell.execute_reply":"2024-04-11T12:55:17.803808Z","shell.execute_reply.started":"2024-04-11T12:55:17.798428Z"},"trusted":true},"outputs":[],"source":["# train_path = \"/kaggle/input/fer2013pluscleanedaugmballanced1/train\"\n","# test_path = \"/kaggle/input/fer2013pluscleanedaugmballanced1/test\"\n","train_path = \"train\"\n","test_path = \"test\""]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T12:55:17.806601Z","iopub.status.busy":"2024-04-11T12:55:17.806261Z","iopub.status.idle":"2024-04-11T12:55:17.829558Z","shell.execute_reply":"2024-04-11T12:55:17.828524Z","shell.execute_reply.started":"2024-04-11T12:55:17.806575Z"},"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/train'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Showing different categories in the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m categories_train \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m categories_test \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(test_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(categories_train)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/train'"]}],"source":["# Showing different categories in the dataset\n","categories_train = os.listdir(train_path)\n","categories_test = os.listdir(test_path)\n","print(categories_train)\n","print(categories_test)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T12:55:17.832139Z","iopub.status.busy":"2024-04-11T12:55:17.831816Z","iopub.status.idle":"2024-04-11T12:56:51.047868Z","shell.execute_reply":"2024-04-11T12:56:51.046726Z","shell.execute_reply.started":"2024-04-11T12:55:17.832111Z"},"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/fer2013pluscleanedaugmballanced1/train'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m images, categories\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load images and labels from train and test folders\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m training_images, training_labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m test_images, test_labels \u001b[38;5;241m=\u001b[39m load_images(test_path)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#list to numpy array\u001b[39;00m\n","Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mload_images\u001b[0;34m(folder)\u001b[0m\n\u001b[1;32m      3\u001b[0m images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m categories \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m emotion_folder \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      7\u001b[0m     category \u001b[38;5;241m=\u001b[39m emotion_folder\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, emotion_folder)):\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/fer2013pluscleanedaugmballanced1/train'"]}],"source":["# Function to load images from a folder\n","def load_images(folder):\n","    images = []\n","    categories = []\n","\n","    for emotion_folder in os.listdir(folder):\n","        category = emotion_folder\n","        for filename in os.listdir(os.path.join(folder, emotion_folder)):\n","            img = cv2.imread(os.path.join(folder, emotion_folder, filename), cv2.IMREAD_GRAYSCALE)\n","            if img is not None:\n","                images.append(img)\n","                categories.append(category)\n","    return images, categories\n","\n","# Load images and labels from train and test folders\n","training_images, training_labels = load_images(train_path)\n","test_images, test_labels = load_images(test_path)\n","\n","\n","#list to numpy array\n","training_images = np.array(training_images)\n","training_labels = np.array(training_labels)\n","test_images = np.array(test_images)\n","test_labels = np.array(test_labels)\n","\n","\n","# Print the total number of images\n","print(\"Total number of training images:\", len(training_images))\n","print(\"Total number of test images:\", len(test_images))"]},{"cell_type":"markdown","metadata":{},"source":["## Show some Images in both Training and Test Set"]},{"cell_type":"markdown","metadata":{},"source":["### Training Set"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T12:56:51.050054Z","iopub.status.busy":"2024-04-11T12:56:51.049322Z","iopub.status.idle":"2024-04-11T12:56:56.329963Z","shell.execute_reply":"2024-04-11T12:56:56.328704Z","shell.execute_reply.started":"2024-04-11T12:56:51.050013Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'training_images' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m sample_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Select sample images and class names\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m sample_images, label_names \u001b[38;5;241m=\u001b[39m select_sample(\u001b[43mtraining_images\u001b[49m, training_labels, sample_num)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Reshape training_images to have an additional dimension for grayscale channel\u001b[39;00m\n\u001b[1;32m     16\u001b[0m training_images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(training_images, (\u001b[38;5;28mlen\u001b[39m(training_images), \u001b[38;5;241m48\u001b[39m, \u001b[38;5;241m48\u001b[39m))\n","\u001b[0;31mNameError\u001b[0m: name 'training_images' is not defined"]}],"source":["# Function to select sample images from each class\n","def select_sample(images, labels, sample_num=5):\n","    sample_images = []\n","    label_names = np.unique(labels)\n","    for label_name in label_names:\n","        label_indices = np.where(labels == label_name)[0][:sample_num]\n","        sample_images.extend(images[label_indices])\n","    return sample_images, label_names\n","\n","sample_num = 10\n","\n","# Select sample images and class names\n","sample_images, label_names = select_sample(training_images, training_labels, sample_num)\n","\n","# Reshape training_images to have an additional dimension for grayscale channel\n","training_images = np.reshape(training_images, (len(training_images), 48, 48))\n","\n","# Plot sample images in a grid\n","num_labels = len(label_names)\n","fig, axes = plt.subplots(num_labels, sample_num, figsize=(15, 15))\n","for i, class_name in enumerate(label_names):\n","    class_indices = np.where(training_labels == class_name)[0][:sample_num]\n","    for j, idx in enumerate(class_indices):\n","        axes[i, j].imshow(training_images[idx], cmap='gray')\n","        axes[i, j].axis('off')\n","        axes[i, j].set_title(class_name, fontsize=10)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Test Set"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T12:56:56.331776Z","iopub.status.busy":"2024-04-11T12:56:56.331222Z","iopub.status.idle":"2024-04-11T12:57:01.556408Z","shell.execute_reply":"2024-04-11T12:57:01.555079Z","shell.execute_reply.started":"2024-04-11T12:56:56.331745Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'test_images' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m sample_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Select sample images and class names\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m sample_images, label_names \u001b[38;5;241m=\u001b[39m select_sample(\u001b[43mtest_images\u001b[49m, test_labels, sample_num)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Reshape training_images to have an additional dimension for grayscale channel\u001b[39;00m\n\u001b[1;32m      7\u001b[0m test_images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(test_images, (\u001b[38;5;28mlen\u001b[39m(test_images), \u001b[38;5;241m48\u001b[39m, \u001b[38;5;241m48\u001b[39m))\n","\u001b[0;31mNameError\u001b[0m: name 'test_images' is not defined"]}],"source":["sample_num = 10\n","\n","# Select sample images and class names\n","sample_images, label_names = select_sample(test_images, test_labels, sample_num)\n","\n","# Reshape training_images to have an additional dimension for grayscale channel\n","test_images = np.reshape(test_images, (len(test_images), 48, 48))\n","\n","# Plot sample images in a grid\n","num_labels = len(label_names)\n","fig, axes = plt.subplots(num_labels, sample_num, figsize = (15, 15))\n","for i, class_name in enumerate(label_names):\n","    class_indices = np.where(test_labels == class_name)[0][:sample_num]\n","    for j, idx in enumerate(class_indices):\n","        axes[i, j].imshow(test_images[idx], cmap='gray')\n","        axes[i, j].axis('off')\n","        axes[i, j].set_title(class_name, fontsize = 10)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Data Visualization"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T12:57:01.558992Z","iopub.status.busy":"2024-04-11T12:57:01.558596Z","iopub.status.idle":"2024-04-11T12:57:03.594286Z","shell.execute_reply":"2024-04-11T12:57:03.593128Z","shell.execute_reply.started":"2024-04-11T12:57:01.558957Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'training_labels' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate sum of images for each class in training set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m training_count \u001b[38;5;241m=\u001b[39m {class_name: np\u001b[38;5;241m.\u001b[39msum(training_labels \u001b[38;5;241m==\u001b[39m class_name) \u001b[38;5;28;01mfor\u001b[39;00m class_name \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39munique(\u001b[43mtraining_labels\u001b[49m)}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create a bar chart using Plotly Express\u001b[39;00m\n\u001b[1;32m      5\u001b[0m ploting \u001b[38;5;241m=\u001b[39m px\u001b[38;5;241m.\u001b[39mbar(x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(training_count\u001b[38;5;241m.\u001b[39mkeys()), y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(training_count\u001b[38;5;241m.\u001b[39mvalues()), color \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(training_count\u001b[38;5;241m.\u001b[39mkeys()))\n","\u001b[0;31mNameError\u001b[0m: name 'training_labels' is not defined"]}],"source":["\n","# Calculate sum of images for each class in training set\n","training_count = {class_name: np.sum(training_labels == class_name) for class_name in np.unique(training_labels)}\n","\n","# Create a bar chart using Plotly Express\n","ploting = px.bar(x = list(training_count.keys()), y = list(training_count.values()), color = list(training_count.keys()))\n","\n","# Customize the layout of the chart\n","ploting.update_layout(\n","    title = {\n","        'text': \"Total images for each class in training set\",\n","        'x': 0.5,\n","        'xanchor': 'center'\n","    },\n","    xaxis_title = \"Emotion\",\n","    yaxis_title = \"Count\"\n",")\n","ploting.update_traces(texttemplate = '%{y}', textposition='inside')  # Display count labels on bar\n","\n","# Display plot\n","ploting.show()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T12:57:03.596210Z","iopub.status.busy":"2024-04-11T12:57:03.595653Z","iopub.status.idle":"2024-04-11T12:57:03.698214Z","shell.execute_reply":"2024-04-11T12:57:03.696865Z","shell.execute_reply.started":"2024-04-11T12:57:03.596181Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'test_labels' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate sum of images for each class in training set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_count \u001b[38;5;241m=\u001b[39m {class_name: np\u001b[38;5;241m.\u001b[39msum(test_labels \u001b[38;5;241m==\u001b[39m class_name) \u001b[38;5;28;01mfor\u001b[39;00m class_name \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39munique(\u001b[43mtest_labels\u001b[49m)}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create a bar chart using Plotly Express\u001b[39;00m\n\u001b[1;32m      5\u001b[0m ploting \u001b[38;5;241m=\u001b[39m px\u001b[38;5;241m.\u001b[39mbar(x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(test_count\u001b[38;5;241m.\u001b[39mkeys()), y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(test_count\u001b[38;5;241m.\u001b[39mvalues()), color \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(test_count\u001b[38;5;241m.\u001b[39mkeys()))\n","\u001b[0;31mNameError\u001b[0m: name 'test_labels' is not defined"]}],"source":["# Calculate sum of images for each class in training set\n","test_count = {class_name: np.sum(test_labels == class_name) for class_name in np.unique(test_labels)}\n","\n","# Create a bar chart using Plotly Express\n","ploting = px.bar(x = list(test_count.keys()), y = list(test_count.values()), color = list(test_count.keys()))\n","\n","# Customize the layout of the chart\n","ploting.update_layout(\n","    title = {\n","        'text': \"Total images for each class in test set\",\n","        'x': 0.5,\n","        'xanchor': 'center'\n","    },\n","    xaxis_title = \"Emotion\",\n","    yaxis_title = \"Count\"\n",")\n","ploting.update_traces(texttemplate = '%{y}', textposition='inside')  # Display count labels on bar\n","\n","# Display plot\n","ploting.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Model Architecture"]},{"cell_type":"markdown","metadata":{},"source":["## Customized VGG16 Model"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T12:57:03.699890Z","iopub.status.busy":"2024-04-11T12:57:03.699563Z","iopub.status.idle":"2024-04-11T12:57:03.705739Z","shell.execute_reply":"2024-04-11T12:57:03.704580Z","shell.execute_reply.started":"2024-04-11T12:57:03.699864Z"},"trusted":true},"outputs":[],"source":["# Set the batch size for training\n","new_batch_size = 128\n","\n","# Define the dropout rate for regularization\n","dropout_rate = 0.5\n","\n","# Specify the number of frozen layers in the model\n","frozen_layer = 170\n","\n","# Set the learning rate for the Adam optimizer\n","adam_lr = 0.001\n","\n","# Set the learning rate for the SGD optimizer\n","sgd_lr = 0.01\n","\n","# Specify the target size for resizing input images\n","img_resize = 64"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T12:57:03.710338Z","iopub.status.busy":"2024-04-11T12:57:03.709959Z","iopub.status.idle":"2024-04-11T12:57:04.836755Z","shell.execute_reply":"2024-04-11T12:57:04.835546Z","shell.execute_reply.started":"2024-04-11T12:57:03.710294Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'VGG16' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Source : https://github.com/rcmalli/keras-vggface\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Source : https://www.codemag.com/Article/2205081/Implementing-Face-Recognition-Using-Deep-Learning-and-Support-Vector-Machines\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# The original VGGFace uses the VGG16 model, which is a convolutional neural network with 16 layers\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create the VGG16 model with specified input shape and average pooling\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m vgg_notop \u001b[38;5;241m=\u001b[39m \u001b[43mVGG16\u001b[49m(input_shape \u001b[38;5;241m=\u001b[39m (img_resize, img_resize, \u001b[38;5;241m3\u001b[39m), include_top \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, pooling \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Get the output of the last layer of the VGG16 model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m last_layer \u001b[38;5;241m=\u001b[39m vgg_notop\u001b[38;5;241m.\u001b[39moutput\n","\u001b[0;31mNameError\u001b[0m: name 'VGG16' is not defined"]}],"source":["# Source : https://github.com/rcmalli/keras-vggface\n","# Source : https://www.codemag.com/Article/2205081/Implementing-Face-Recognition-Using-Deep-Learning-and-Support-Vector-Machines\n","# The original VGGFace uses the VGG16 model, which is a convolutional neural network with 16 layers\n","\n","# Create the VGG16 model with specified input shape and average pooling\n","vgg_notop = VGG16(input_shape = (img_resize, img_resize, 3), include_top = False, pooling = 'avg')\n","\n","# Get the output of the last layer of the VGG16 model\n","last_layer = vgg_notop.output\n","\n","# Flatten the output\n","x = Flatten(name = 'flatten')(last_layer)\n","\n","# Apply Dropout regularization\n","x = Dropout(dropout_rate)(x)\n","\n","# Add a fully connected layer with 4096 units and ReLU activation\n","x = Dense(4096, activation = 'relu', name = 'fc6')(x)\n","\n","# Apply Dropout regularization\n","x = Dropout(dropout_rate)(x)\n","\n","# Add another fully connected layer with 1024 units and ReLU activation\n","x = Dense(1024, activation = 'relu', name = 'fc7')(x)\n","\n","# Apply Dropout regularization\n","x = Dropout(dropout_rate)(x)\n","\n","# Define the indices of layers to freeze during training\n","batch_norm_indices = [2, 6, 9, 13, 14, 18, 21, 24, 28, 31, 34, 38, 41, 45, 46, 53, 56, 60, 63, 66, 70, 73, 76, 80, 83, 87, 88, 92, 95, 98, 102, 105, 108, 112, 115, 118, 122, 125, 128, 132, 135, 138, 142, 145, 149, 150, 154, 157, 160, 164, 167, 170]\n","\n","# Freeze the specified layers by setting their trainable attribute to False\n","for i in range(frozen_layer):\n","    if i < len(vgg_notop.layers) and i not in batch_norm_indices:\n","        vgg_notop.layers[i].trainable = False\n","\n","# Add a dense layer with 7 units and softmax activation as the classifier layer\n","x = Dense(7, activation = 'softmax', name = 'classifier')(x)\n","\n","# Create the vgg16 model with the modified layers\n","vgg16_model = Model(vgg_notop.input, x)\n","\n","\n","optim = tf.keras.optimizers.Adam(learning_rate=adam_lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","\n","sgd = tf.keras.optimizers.SGD(learning_rate=sgd_lr, momentum=0.9, nesterov=True)\n","\n","# Compile the model with the specified optimizer, loss function, and metrics\n","vgg16_model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","\n","#model summary\n","vgg16_model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["# Create Data Augmentation"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T12:57:04.838160Z","iopub.status.busy":"2024-04-11T12:57:04.837828Z","iopub.status.idle":"2024-04-11T12:57:04.844911Z","shell.execute_reply":"2024-04-11T12:57:04.843928Z","shell.execute_reply.started":"2024-04-11T12:57:04.838133Z"},"trusted":true},"outputs":[],"source":["# Source : https://www.geeksforgeeks.org/python-data-augmentation/\n","# Source : https://www.analyticsvidhya.com/blog/2020/08/image-augmentation-on-the-fly-using-keras-imagedatagenerator/\n","train_datagen = ImageDataGenerator(\n","    rescale = 1 / 255.,        # Rescale pixel values to be between 0 and 1\n","    rotation_range = 10,       # Rotates image with the number of degrees that we assigned\n","    zoom_range = 0.2,          # Zoom to our object randomly\n","    horizontal_flip = True,    # Randomly flip images horizontally\n","    height_shift_range = 0.1,  # Shift the height of images by up to 10% randomly\n","    width_shift_range = 0.1,   # Shift the width of images by up to 10% randomly\n","    fill_mode = 'nearest',     # Replaces the empty area with the nearest pixel values\n","    validation_split=0.2    # set the validation split\n",")\n","\n","test_datagen = ImageDataGenerator(\n","    rescale = 1 / 255.\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Apply Augmentation to the images"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T12:57:04.846678Z","iopub.status.busy":"2024-04-11T12:57:04.846172Z","iopub.status.idle":"2024-04-11T12:57:08.513895Z","shell.execute_reply":"2024-04-11T12:57:08.512724Z","shell.execute_reply.started":"2024-04-11T12:57:04.846650Z"},"trusted":true},"outputs":[],"source":["# Train generator\n","print('Generate Training Set - ',end = ' ')\n","train_generator = train_datagen.flow_from_directory(\n","    directory = train_path,\n","    class_mode = \"categorical\",\n","    target_size = (img_resize,img_resize),\n","    color_mode = 'rgb',\n","    batch_size = new_batch_size,\n","    shuffle = True,\n","    subset = 'training'\n","\n",")\n","\n","print('Generate Validation Set - ',end = ' ')\n","validation_generator = train_datagen.flow_from_directory(\n","    directory = train_path,\n","    target_size = (img_resize,img_resize),\n","    batch_size = new_batch_size,\n","    class_mode = 'categorical',\n","    color_mode = 'rgb',\n","    shuffle = False,\n","    subset = 'validation' # set as validation data\n","   )\n","\n","# Test generator\n","print('Generate Shuffled Test Set - ', end = ' ')\n","test_generator = test_datagen.flow_from_directory(\n","    directory = test_path,\n","    class_mode = 'categorical',\n","    target_size = (img_resize,img_resize),\n","    color_mode = 'rgb',\n","    batch_size = new_batch_size,\n","    shuffle = True\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Create Callbacks"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T12:57:08.515558Z","iopub.status.busy":"2024-04-11T12:57:08.515172Z","iopub.status.idle":"2024-04-11T12:57:08.521860Z","shell.execute_reply":"2024-04-11T12:57:08.520753Z","shell.execute_reply.started":"2024-04-11T12:57:08.515523Z"},"trusted":true},"outputs":[],"source":["steps_per_epoch = train_generator.n // train_generator.batch_size\n","print(steps_per_epoch)\n","\n","validate_steps = validation_generator.n // validation_generator.batch_size\n","print(validate_steps)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T12:57:08.524155Z","iopub.status.busy":"2024-04-11T12:57:08.523568Z","iopub.status.idle":"2024-04-11T12:57:08.541277Z","shell.execute_reply":"2024-04-11T12:57:08.540142Z","shell.execute_reply.started":"2024-04-11T12:57:08.524037Z"},"trusted":true},"outputs":[],"source":["# Source : https://keras.io/api/callbacks/model_checkpoint/\n","checkpoint_filepath = \"new_finalVGG16_weight.weights.h5\"\n","\n","# Save model weights\n","checkpoint_callback = ModelCheckpoint(\n","    filepath = checkpoint_filepath,\n","    monitor = 'val_accuracy',\n","    save_best_only = True,\n","    save_weights_only = True,\n","    mode = 'max',\n","    verbose = 1\n",")\n","\n","# Source : https://keras.io/api/callbacks/reduce_lr_on_plateau/\n","# Create ReduceLROnPlateau Callback to reduce overfitting by decreasing learning if the model loss does not decrease\n","reduce_lr = ReduceLROnPlateau(\n","    monitor = 'val_loss',\n","    factor = 0.5,\n","    patience = 10,\n","    min_lr = 0.00001, \n","    verbose = 1\n",")\n","\n","# Source : https://keras.io/api/callbacks/early_stopping/\n","# Create Early Stopping Callback to monitor the accuracy\n","early_stopping = EarlyStopping(\n","    monitor = 'val_loss',\n","    min_delta = 0.005,\n","    patience = 10,\n","    verbose = 1,\n","    restore_best_weights = True\n",")\n","\n","callbacks = [checkpoint_callback, reduce_lr, early_stopping]"]},{"cell_type":"markdown","metadata":{},"source":["# Model Training"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T13:32:18.936280Z","iopub.status.busy":"2024-04-08T13:32:18.935535Z","iopub.status.idle":"2024-04-08T13:32:18.951502Z","shell.execute_reply":"2024-04-08T13:32:18.950259Z","shell.execute_reply.started":"2024-04-08T13:32:18.936242Z"},"trusted":true},"outputs":[],"source":["# vgg16_model_history = vgg16_model.fit(\n","#     train_generator,\n","#     validation_data = validation_generator,\n","#     epochs = 300,\n","#     callbacks = callbacks,\n","#     steps_per_epoch = steps_per_epoch,\n","#     validation_steps = validate_steps,\n","#     verbose = 1\n","# )"]},{"cell_type":"markdown","metadata":{},"source":["# Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get the training and validation accuracy from the history\n","train_acc = vgg16_model_history.history['accuracy']\n","val_acc = vgg16_model_history.history['val_accuracy']\n","\n","# Plot the training and validation accuracy\n","plt.plot(train_acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["<img src=attachment:927c14c4-ffb6-4ec4-865d-a9299c68c824.png width=\"500\">"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get the training and validation accuracy from the history\n","train_acc = vgg16_model_history.history['loss']\n","val_acc = vgg16_model_history.history['val_loss']\n","\n","# Plot the training and validation accuracy\n","plt.plot(train_acc, label='Training Loss')\n","plt.plot(val_acc, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["<img src=attachment:e3bfef39-4e61-4fde-9d55-3fec4a7e67b7.png width=\"500\">"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import h5py\n","# # Save the model\n","# vgg16_model.save(\"/kaggle/working/new_finalcustomedVGG16_model.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T12:57:08.543106Z","iopub.status.busy":"2024-04-11T12:57:08.542771Z","iopub.status.idle":"2024-04-11T12:57:09.793786Z","shell.execute_reply":"2024-04-11T12:57:09.792704Z","shell.execute_reply.started":"2024-04-11T12:57:08.543078Z"},"trusted":true},"outputs":[],"source":["model_path = \"/kaggle/input/mine-vgg16/validation = 0.2/finalcustomedVGG16_model.h5\"\n","final_vgg16_model = load_model(model_path, compile=False)\n","\n","final_vgg16_model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","final_vgg16_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T12:57:09.795774Z","iopub.status.busy":"2024-04-11T12:57:09.795309Z","iopub.status.idle":"2024-04-11T13:04:32.625229Z","shell.execute_reply":"2024-04-11T13:04:32.624055Z","shell.execute_reply.started":"2024-04-11T12:57:09.795732Z"},"trusted":true},"outputs":[],"source":["vgg16_Score1 = final_vgg16_model.evaluate(train_generator)\n","\n","print(\"    Test Loss: {:.5f}\".format(vgg16_Score1[0]))\n","print(\"Train Accuracy: {:.2f}%\".format(vgg16_Score1[1] * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:04:32.627533Z","iopub.status.busy":"2024-04-11T13:04:32.627069Z","iopub.status.idle":"2024-04-11T13:06:54.863289Z","shell.execute_reply":"2024-04-11T13:06:54.862070Z","shell.execute_reply.started":"2024-04-11T13:04:32.627494Z"},"trusted":true},"outputs":[],"source":["vgg16_Score2 = final_vgg16_model.evaluate(validation_generator)\n","\n","print(\"    Test Loss: {:.5f}\".format(vgg16_Score2[0]))\n","print(\"Validation Accuracy: {:.2f}%\".format(vgg16_Score2[1] * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:06:54.865179Z","iopub.status.busy":"2024-04-11T13:06:54.864778Z","iopub.status.idle":"2024-04-11T13:08:50.491594Z","shell.execute_reply":"2024-04-11T13:08:50.490416Z","shell.execute_reply.started":"2024-04-11T13:06:54.865145Z"},"trusted":true},"outputs":[],"source":["vgg16_Score3 = final_vgg16_model.evaluate(test_generator)\n","\n","print(\"    Test Loss: {:.5f}\".format(vgg16_Score3[0]))\n","print(\"Test Accuracy: {:.2f}%\".format(vgg16_Score3[1] * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:08:50.493878Z","iopub.status.busy":"2024-04-11T13:08:50.493227Z","iopub.status.idle":"2024-04-11T13:10:45.940524Z","shell.execute_reply":"2024-04-11T13:10:45.939393Z","shell.execute_reply.started":"2024-04-11T13:08:50.493839Z"},"trusted":true},"outputs":[],"source":["customized_vgg16_labels = test_generator.classes\n","\n","customized_vgg16_Prediction = final_vgg16_model.predict(test_generator)\n","\n","customized_vgg16_Prediction_labels = np.argmax(customized_vgg16_Prediction, axis = 1)\n","\n","# Compute the confusion matrix\n","class_list = list(train_generator.class_indices)\n","cnf_mtx = confusion_matrix(customized_vgg16_labels, customized_vgg16_Prediction_labels)\n","sns.set()\n","sns.heatmap(\n","    cnf_mtx,\n","    annot = True,\n","    fmt = 'd',\n","    cmap = 'YlGnBu',\n","    xticklabels = class_list,\n","    yticklabels = class_list\n",")\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.title('Customized VGG16 Confusion Matrix')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Face Emotion Detection System"]},{"cell_type":"markdown","metadata":{},"source":["## Demonstration on images"]},{"cell_type":"markdown","metadata":{},"source":["### First Example"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:45.942164Z","iopub.status.busy":"2024-04-11T13:10:45.941819Z","iopub.status.idle":"2024-04-11T13:10:46.021865Z","shell.execute_reply":"2024-04-11T13:10:46.020582Z","shell.execute_reply.started":"2024-04-11T13:10:45.942134Z"},"trusted":true},"outputs":[],"source":["image_path = '/kaggle/input/testing-image/Testing-sameple/happyboy.jpg'\n","frame = cv2.imread(image_path)\n","frame.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:46.023870Z","iopub.status.busy":"2024-04-11T13:10:46.023129Z","iopub.status.idle":"2024-04-11T13:10:47.152487Z","shell.execute_reply":"2024-04-11T13:10:47.151362Z","shell.execute_reply.started":"2024-04-11T13:10:46.023833Z"},"trusted":true},"outputs":[],"source":["plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:47.154783Z","iopub.status.busy":"2024-04-11T13:10:47.153848Z","iopub.status.idle":"2024-04-11T13:10:47.195666Z","shell.execute_reply":"2024-04-11T13:10:47.194351Z","shell.execute_reply.started":"2024-04-11T13:10:47.154740Z"},"trusted":true},"outputs":[],"source":["# Face detection algorithm that ignore the background, only crop the face (also convert it into gray image)\n","face_cascade = cv2.CascadeClassifier('/kaggle/input/squres/haarcascade_frontalface_default.xml')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:47.197359Z","iopub.status.busy":"2024-04-11T13:10:47.197010Z","iopub.status.idle":"2024-04-11T13:10:47.208290Z","shell.execute_reply":"2024-04-11T13:10:47.206866Z","shell.execute_reply.started":"2024-04-11T13:10:47.197329Z"},"trusted":true},"outputs":[],"source":["gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","gray.shape"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:47.210355Z","iopub.status.busy":"2024-04-11T13:10:47.209602Z","iopub.status.idle":"2024-04-11T13:10:47.931646Z","shell.execute_reply":"2024-04-11T13:10:47.930312Z","shell.execute_reply.started":"2024-04-11T13:10:47.210317Z"},"trusted":true},"outputs":[],"source":["# Source : https://docs.opencv.org/3.1.0/d7/d8b/tutorial_py_face_detection.html#gsc.tab=0\n","# Source : https://stackoverflow.com/questions/36218385/parameters-of-detectmultiscale-in-opencv-using-python\n","# It is used to find all the possible faces in the video or image\n","faces = face_cascade.detectMultiScale(gray, scaleFactor = 1.1, minNeighbors = 5)\n","for (x, y, w, h) in faces:\n","    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","    face = gray[y:y + h, x:x + w]\n","    colour = frame[y:y + h, x:x + w]\n","    faces = face_cascade.detectMultiScale(face)\n","    # If no faces showed in the image or video, it will throw a error message\n","    if len(faces) == 0:\n","        print(\"Do not detect face\")\n","    else:\n","        for (ex, ey, ew, eh) in faces:\n","            face_roi = colour[ey:ey + eh, ex:ex + ew]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:47.933494Z","iopub.status.busy":"2024-04-11T13:10:47.932991Z","iopub.status.idle":"2024-04-11T13:10:49.056163Z","shell.execute_reply":"2024-04-11T13:10:49.054767Z","shell.execute_reply.started":"2024-04-11T13:10:47.933450Z"},"trusted":true},"outputs":[],"source":["plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:49.058290Z","iopub.status.busy":"2024-04-11T13:10:49.057891Z","iopub.status.idle":"2024-04-11T13:10:49.679236Z","shell.execute_reply":"2024-04-11T13:10:49.677999Z","shell.execute_reply.started":"2024-04-11T13:10:49.058255Z"},"trusted":true},"outputs":[],"source":["plt.imshow(cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB))"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:49.681424Z","iopub.status.busy":"2024-04-11T13:10:49.680915Z","iopub.status.idle":"2024-04-11T13:10:49.688069Z","shell.execute_reply":"2024-04-11T13:10:49.686384Z","shell.execute_reply.started":"2024-04-11T13:10:49.681386Z"},"trusted":true},"outputs":[],"source":["final_image = cv2.resize(face_roi, (64, 64))\n","final_image = np.expand_dims(final_image, axis = 0)\n","final_image = final_image/255.0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:49.698073Z","iopub.status.busy":"2024-04-11T13:10:49.697099Z","iopub.status.idle":"2024-04-11T13:10:50.018842Z","shell.execute_reply":"2024-04-11T13:10:50.017418Z","shell.execute_reply.started":"2024-04-11T13:10:49.697974Z"},"trusted":true},"outputs":[],"source":["Predictions = final_vgg16_model.predict(final_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:50.021748Z","iopub.status.busy":"2024-04-11T13:10:50.020546Z","iopub.status.idle":"2024-04-11T13:10:50.030449Z","shell.execute_reply":"2024-04-11T13:10:50.029339Z","shell.execute_reply.started":"2024-04-11T13:10:50.021672Z"},"trusted":true},"outputs":[],"source":["Predictions[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:50.033164Z","iopub.status.busy":"2024-04-11T13:10:50.032563Z","iopub.status.idle":"2024-04-11T13:10:50.042049Z","shell.execute_reply":"2024-04-11T13:10:50.041027Z","shell.execute_reply.started":"2024-04-11T13:10:50.033119Z"},"trusted":true},"outputs":[],"source":["array = np.array([8.8420827e-03, 2.5103660e-04, 1.4054395e-03, 9.5446646e-01,\n","                  1.0098061e-02, 1.0218234e-02, 1.4718594e-02])\n","\n","# Find the index of the maximum value in the array\n","max_index = np.argmax(array)\n","\n","# Retrieve the maximum value from the array\n","max_value = array[max_index]\n","\n","print(\"The largest number is:\", max_value)\n","print(\"Its index in the array is:\", max_index)"]},{"cell_type":"markdown","metadata":{},"source":["As here we can see that \"9.5446646e-01\" is the largest number in the array."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:50.043957Z","iopub.status.busy":"2024-04-11T13:10:50.043542Z","iopub.status.idle":"2024-04-11T13:10:50.054584Z","shell.execute_reply":"2024-04-11T13:10:50.053608Z","shell.execute_reply.started":"2024-04-11T13:10:50.043921Z"},"trusted":true},"outputs":[],"source":["np.argmax(Predictions)"]},{"cell_type":"markdown","metadata":{},"source":["In the fer2013 dataset: 0 = angry 1 = disgust 2 = fear 3 = happy 4 = neutral 5 = sad 6 = surprise\n","\n","The code showing 3, which means the model predict that the image is equals to \"happy\". It is a correct predict as the image is \"happy boy\"."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:50.057008Z","iopub.status.busy":"2024-04-11T13:10:50.056142Z","iopub.status.idle":"2024-04-11T13:10:50.108934Z","shell.execute_reply":"2024-04-11T13:10:50.107624Z","shell.execute_reply.started":"2024-04-11T13:10:50.056945Z"},"trusted":true},"outputs":[],"source":["image2_path = '/kaggle/input/testing-image/Testing-sameple/surprised_man.jpeg'\n","frame2 = cv2.imread(image2_path)\n","frame2.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:50.111045Z","iopub.status.busy":"2024-04-11T13:10:50.110608Z","iopub.status.idle":"2024-04-11T13:10:50.812458Z","shell.execute_reply":"2024-04-11T13:10:50.810941Z","shell.execute_reply.started":"2024-04-11T13:10:50.111007Z"},"trusted":true},"outputs":[],"source":["plt.imshow(cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:50.814046Z","iopub.status.busy":"2024-04-11T13:10:50.813723Z","iopub.status.idle":"2024-04-11T13:10:50.823044Z","shell.execute_reply":"2024-04-11T13:10:50.821821Z","shell.execute_reply.started":"2024-04-11T13:10:50.814018Z"},"trusted":true},"outputs":[],"source":["gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n","gray2.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:50.824945Z","iopub.status.busy":"2024-04-11T13:10:50.824562Z","iopub.status.idle":"2024-04-11T13:10:51.799238Z","shell.execute_reply":"2024-04-11T13:10:51.798058Z","shell.execute_reply.started":"2024-04-11T13:10:50.824911Z"},"trusted":true},"outputs":[],"source":["faces = face_cascade.detectMultiScale(gray2, scaleFactor = 1.1, minNeighbors = 5)\n","for (x, y, w, h) in faces:\n","    cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","    face = gray2[y:y + h, x:x + w]\n","    colour = frame2[y:y + h, x:x + w]\n","    faces = face_cascade.detectMultiScale(face)\n","    # If no faces showed in the image or video, it will throw a error message\n","    if len(faces) == 0:\n","        print(\"Do not detect face\")\n","    else:\n","        for (ex, ey, ew, eh) in faces:\n","            face_roi = colour[ey:ey + eh, ex:ex + ew]\n","\n","plt.imshow(cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:51.800909Z","iopub.status.busy":"2024-04-11T13:10:51.800556Z","iopub.status.idle":"2024-04-11T13:10:52.244149Z","shell.execute_reply":"2024-04-11T13:10:52.242982Z","shell.execute_reply.started":"2024-04-11T13:10:51.800879Z"},"trusted":true},"outputs":[],"source":["plt.imshow(cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:52.246721Z","iopub.status.busy":"2024-04-11T13:10:52.245755Z","iopub.status.idle":"2024-04-11T13:10:52.351814Z","shell.execute_reply":"2024-04-11T13:10:52.350646Z","shell.execute_reply.started":"2024-04-11T13:10:52.246666Z"},"trusted":true},"outputs":[],"source":["final_image2 = cv2.resize(face_roi, (64, 64))\n","final_image2 = np.expand_dims(final_image2, axis = 0)\n","final_image2 = final_image2/255.0\n","\n","Predictions2 = final_vgg16_model.predict(final_image2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:52.353755Z","iopub.status.busy":"2024-04-11T13:10:52.353203Z","iopub.status.idle":"2024-04-11T13:10:52.361666Z","shell.execute_reply":"2024-04-11T13:10:52.360458Z","shell.execute_reply.started":"2024-04-11T13:10:52.353662Z"},"trusted":true},"outputs":[],"source":["Predictions2[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:52.363393Z","iopub.status.busy":"2024-04-11T13:10:52.363059Z","iopub.status.idle":"2024-04-11T13:10:52.371216Z","shell.execute_reply":"2024-04-11T13:10:52.370180Z","shell.execute_reply.started":"2024-04-11T13:10:52.363365Z"},"trusted":true},"outputs":[],"source":["array = np.array([2.8438936e-04, 2.5721829e-06, 2.8893199e-02, 5.6542992e-04,\n","                  6.1503989e-03, 1.5998984e-03, 9.6250409e-01])\n","\n","# Find the index of the maximum value in the array\n","max_index = np.argmax(array)\n","\n","# Retrieve the maximum value from the array\n","max_value = array[max_index]\n","\n","print(\"The largest number is:\", max_value)\n","print(\"Its index in the array is:\", max_index)"]},{"cell_type":"markdown","metadata":{},"source":["As here we can see that \"9.6250409e-01\" is the largest number in the array."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:52.373451Z","iopub.status.busy":"2024-04-11T13:10:52.372942Z","iopub.status.idle":"2024-04-11T13:10:52.397525Z","shell.execute_reply":"2024-04-11T13:10:52.396547Z","shell.execute_reply.started":"2024-04-11T13:10:52.373417Z"},"trusted":true},"outputs":[],"source":["np.argmax(Predictions2)"]},{"cell_type":"markdown","metadata":{},"source":["The code showing 6, which means the model predict that the image is equals to \"surprise\". It is a correct predict as the image is \"surprised man\"."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:52.399484Z","iopub.status.busy":"2024-04-11T13:10:52.399100Z","iopub.status.idle":"2024-04-11T13:10:52.616416Z","shell.execute_reply":"2024-04-11T13:10:52.615242Z","shell.execute_reply.started":"2024-04-11T13:10:52.399445Z"},"trusted":true},"outputs":[],"source":["image3_path = '/kaggle/input/multiple-people/Screenshot 2024-04-06 at 1.07.09AM.png'\n","frame3 = cv2.imread(image3_path)\n","frame3.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:52.618543Z","iopub.status.busy":"2024-04-11T13:10:52.617893Z","iopub.status.idle":"2024-04-11T13:10:54.328545Z","shell.execute_reply":"2024-04-11T13:10:54.327744Z","shell.execute_reply.started":"2024-04-11T13:10:52.618503Z"},"trusted":true},"outputs":[],"source":["plt.imshow(cv2.cvtColor(frame3, cv2.COLOR_BGR2RGB))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:54.330389Z","iopub.status.busy":"2024-04-11T13:10:54.330043Z","iopub.status.idle":"2024-04-11T13:10:54.339004Z","shell.execute_reply":"2024-04-11T13:10:54.338113Z","shell.execute_reply.started":"2024-04-11T13:10:54.330352Z"},"trusted":true},"outputs":[],"source":["gray3 = cv2.cvtColor(frame3, cv2.COLOR_BGR2GRAY)\n","gray3.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:54.340807Z","iopub.status.busy":"2024-04-11T13:10:54.340464Z","iopub.status.idle":"2024-04-11T13:10:57.039809Z","shell.execute_reply":"2024-04-11T13:10:57.038637Z","shell.execute_reply.started":"2024-04-11T13:10:54.340780Z"},"trusted":true},"outputs":[],"source":["faces = face_cascade.detectMultiScale(gray3, scaleFactor = 1.29, minNeighbors = 6, minSize = (50,50))\n","for (x, y, w, h) in faces:\n","    cv2.rectangle(frame3, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","    face = gray3[y:y + h, x:x + w]\n","    colour = frame3[y:y + h, x:x + w]\n","    faces = face_cascade.detectMultiScale(face)\n","    # If no faces showed in the image or video, it will throw a error message\n","    if len(faces) == 0:\n","        print(\"Do not detect face\")\n","    else:\n","        for (ex, ey, ew, eh) in faces:\n","            face_roi = colour[ey:ey + eh, ex:ex + ew]\n","\n","plt.imshow(cv2.cvtColor(frame3, cv2.COLOR_BGR2RGB))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:57.042277Z","iopub.status.busy":"2024-04-11T13:10:57.041405Z","iopub.status.idle":"2024-04-11T13:10:59.864985Z","shell.execute_reply":"2024-04-11T13:10:59.863719Z","shell.execute_reply.started":"2024-04-11T13:10:57.042230Z"},"trusted":true},"outputs":[],"source":["def predict_face(face_roi):\n","    final_image = cv2.resize(face_roi, (64, 64))\n","    final_image = np.expand_dims(final_image, axis=0)\n","    final_image = final_image / 255.0\n","\n","    predictions = final_vgg16_model.predict(final_image)\n","    # Process the predictions and return the result\n","    # Replace this placeholder return statement with your actual processing logic\n","    return predictions\n","\n","faces = face_cascade.detectMultiScale(gray3, scaleFactor=1.29, minNeighbors=6, minSize = (100,100))\n","\n","for (x, y, w, h) in faces:\n","    cv2.rectangle(frame3, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","    face = gray3[y:y + h, x:x + w]\n","    colour = frame3[y:y + h, x:x + w]\n","    small_faces = face_cascade.detectMultiScale(face)\n","    \n","    if len(small_faces) == 0:\n","        print(\"Do not detect face\")\n","    else:\n","        for (ex, ey, ew, eh) in small_faces:\n","            face_roi = colour[ey:ey + eh, ex:ex + ew]\n","            predictions = predict_face(face_roi)  # Call your prediction function\n","\n","            # Convert the array of predictions to a string representation\n","            prediction_str = ', '.join(str(p) for p in predictions)\n","\n","            plt.imshow(cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB))\n","            plt.title(\"Predictions: \" + prediction_str)\n","            plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Although it can detect 5 of the faces, but 1 of the human faces cannot be detected due to the feature of small face. So haar cascade has a problem that it cannot detect small faces. \n","\n","To solve the problem, lets try dlib frontal."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:10:59.866665Z","iopub.status.busy":"2024-04-11T13:10:59.866326Z","iopub.status.idle":"2024-04-11T13:19:42.892743Z","shell.execute_reply":"2024-04-11T13:19:42.891306Z","shell.execute_reply.started":"2024-04-11T13:10:59.866635Z"},"trusted":true},"outputs":[],"source":["pip install dlib"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:19:42.895470Z","iopub.status.busy":"2024-04-11T13:19:42.895004Z","iopub.status.idle":"2024-04-11T13:19:43.003741Z","shell.execute_reply":"2024-04-11T13:19:43.002817Z","shell.execute_reply.started":"2024-04-11T13:19:42.895428Z"},"trusted":true},"outputs":[],"source":["import dlib"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:19:43.006418Z","iopub.status.busy":"2024-04-11T13:19:43.005748Z","iopub.status.idle":"2024-04-11T13:20:08.138197Z","shell.execute_reply":"2024-04-11T13:20:08.136932Z","shell.execute_reply.started":"2024-04-11T13:19:43.006379Z"},"trusted":true},"outputs":[],"source":["def predict_faces(face_rois):\n","    batch_size = len(face_rois)\n","    if batch_size == 0:\n","        return []\n","\n","    final_images = [cv2.resize(face_roi, (64, 64)) for face_roi in face_rois]\n","    final_images = np.stack(final_images)\n","    final_images = final_images / 255.0\n","\n","    predictions = final_vgg16_model.predict(final_images)\n","    # Process the predictions and return the results\n","    return predictions\n","\n","# Initialize the face detector from Dlib\n","face_detector = dlib.get_frontal_face_detector()\n","face_detector = dlib.cnn_face_detection_model_v1(\"/kaggle/input/fcae-detect/mmod_human_face_detector.dat\")\n","\n","# Load the image\n","frame3 = cv2.imread(\"/kaggle/input/multiple-people/Screenshot 2024-04-06 at 1.07.09AM.png\")\n","\n","# Convert the image to grayscale\n","gray3 = cv2.cvtColor(frame3, cv2.COLOR_BGR2GRAY)\n","\n","# Detect faces in the image\n","faces = face_detector(gray3)\n","\n","face_regions = []\n","\n","scaling_factor = 1.3  # Adjust the scaling factor to increase or decrease the size of the cropped region\n","\n","for face in faces:\n","    x, y, w, h = face.rect.left(), face.rect.top(), face.rect.width(), face.rect.height()\n","    \n","    # Increase the size of the face region using the scaling factor\n","    scaled_w = int(w * scaling_factor)\n","    scaled_h = int(h * scaling_factor)\n","    scaled_x = max(0, x - int((scaled_w - w) / 2))\n","    scaled_y = max(0, y - int((scaled_h - h) / 2))\n","    \n","    face_gray = gray3[scaled_y:scaled_y + scaled_h, scaled_x:scaled_x + scaled_w]\n","    face_color = frame3[scaled_y:scaled_y + scaled_h, scaled_x:scaled_x + scaled_w]\n","    small_faces = face_detector(face_gray, upsample_num_times=0)\n","\n","    if len(small_faces) > 0:\n","        for small_face in small_faces:\n","            ex, ey, ew, eh = small_face.rect.left(), small_face.rect.top(), small_face.rect.width(), small_face.rect.height()\n","            face_roi = face_color[ey:ey + eh, ex:ex + ew]\n","            if face_roi.size != 0:\n","                face_regions.append(face_roi)\n","\n","# Predict the faces in batches\n","batch_size = 10\n","num_faces = len(face_regions)\n","for i in range(0, num_faces, batch_size):\n","    batch_faces = face_regions[i:i + batch_size]\n","    predictions = predict_faces(batch_faces)\n","\n","    for j, prediction in enumerate(predictions):\n","        prediction_str = ', '.join(str(p) for p in prediction)\n","\n","        # Display the face ROI and frame with prediction\n","        plt.figure()\n","        plt.imshow(batch_faces[j][:, :, ::-1])\n","        plt.title(\"Predictions: \" + prediction_str)\n","        plt.show()\n","\n","        # Draw prediction text on the original frame\n","        cv2.putText(frame3, \"Predictions: \" + prediction_str, (x + 5, y - 20),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)"]},{"cell_type":"markdown","metadata":{},"source":["## Demonstration on Video"]},{"cell_type":"markdown","metadata":{},"source":["### Frontal Face Experiment"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T13:57:36.063316Z","iopub.status.busy":"2024-04-08T13:57:36.062958Z","iopub.status.idle":"2024-04-08T13:57:36.070209Z","shell.execute_reply":"2024-04-08T13:57:36.069105Z","shell.execute_reply.started":"2024-04-08T13:57:36.063287Z"},"trusted":true},"outputs":[],"source":["# def predict_faces(face_rois):\n","#     batch_size = len(face_rois)\n","#     if batch_size == 0:\n","#         return []\n","\n","#     final_images = [cv2.resize(face_roi, (64, 64)) for face_roi in face_rois]\n","#     final_images = np.stack(final_images)\n","#     final_images = final_images / 255.0\n","\n","#     predictions = final_vgg16_model.predict(final_images)\n","#     # Process the predictions and return the results\n","#     return predictions\n","\n","# def emotion_recog(frame):\n","#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","#     faces = face_detector(gray)\n","    \n","#     # Predict the emotions for each face\n","#     for face in faces:\n","#         x, y, w, h = face.rect.left(), face.rect.top(), face.rect.right(), face.rect.bottom()\n","#         face_gray = gray[y:y + h, x:x + w]\n","#         face_color = frame[y:y + h, x:x + w]\n","        \n","#         # Resize the face region for prediction\n","#         face_resize = cv2.resize(face_gray, (48, 48))\n","#         face_resize = np.expand_dims(face_resize, axis=0) / 255.0\n","        \n","#         # Predict emotions\n","#         predictions = predict_faces.predict(face_resize)[0]\n","#         emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n","#         pred_label = emotion_labels[np.argmax(predictions)]\n","#         pred_confidence = np.max(predictions)\n","\n","#         # Draw rectangle on the frame\n","#         cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","        \n","#         # Display emotion label with highest confidence\n","#         cv2.putText(frame, f\"Emotion: {pred_label}\", (x, y - 10),\n","#                     cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2, cv2.LINE_AA)\n","#         cv2.putText(frame, f\"Confidence: {pred_confidence:.2f}\", (x, y - 35),\n","#                     cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2, cv2.LINE_AA)\n","    \n","#     return frame"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T13:57:36.072177Z","iopub.status.busy":"2024-04-08T13:57:36.071601Z","iopub.status.idle":"2024-04-08T13:57:36.090797Z","shell.execute_reply":"2024-04-08T13:57:36.089393Z","shell.execute_reply.started":"2024-04-08T13:57:36.072125Z"},"trusted":true},"outputs":[],"source":["# # Initialize the face detector from Dlib\n","# face_detector = dlib.get_frontal_face_detector()\n","# face_detector = dlib.cnn_face_detection_model_v1(\"/kaggle/input/fcae-detect/mmod_human_face_detector.dat\")\n","\n","# # Load the video\n","# cap = cv2.VideoCapture('/kaggle/input/mp4-upload/8159867-hd_1920_1080_25fps.mp4')\n","\n","# ret, frame = cap.read()\n","# frame_height, frame_width, _ = frame.shape\n","\n","# out = cv2.VideoWriter('newtry_multiple_done.avi', cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), 10, (frame_width, frame_height))\n","# print(\"Processing Video......\")\n","\n","# while cap.isOpened():\n","#     ret, frame = cap.read()\n","#     if not ret:\n","#         out.release()\n","#         break\n","    \n","#     output = emotion_recog(frame)\n","#     out.write(output)\n","\n","# out.release()\n","# print(\"Done processing video\")"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T13:57:36.093313Z","iopub.status.busy":"2024-04-08T13:57:36.092937Z","iopub.status.idle":"2024-04-08T13:57:36.109536Z","shell.execute_reply":"2024-04-08T13:57:36.108204Z","shell.execute_reply.started":"2024-04-08T13:57:36.093285Z"},"trusted":true},"outputs":[],"source":["# def emotion_recog(frame):\n","#     emotion_dict = {0: \"Angry\", 1: \"Disgust\", 2: \"Fear\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprise\"}\n","\n","#     # Initialize the face detector from Dlib\n","#     face_detector = dlib.get_frontal_face_detector()\n","\n","#     gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","#     faces = face_detector(gray_frame)\n","\n","#     for face in faces:\n","#         # Extract the bounding box coordinates of the face\n","#         x, y, w, h = face.left(), face.top(), face.width(), face.height()\n","\n","#         cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","#         face_roi = gray_frame[y:y + h, x:x + w]\n","#         face_rgb = cv2.cvtColor(face_roi, cv2.COLOR_GRAY2RGB)\n","#         cropped_img = cv2.resize(face_rgb, (64, 64))\n","#         cropped_img = cropped_img.reshape((1, 64, 64, 3))\n","\n","#         emotion_prediction = final_vgg16_model.predict(cropped_img)\n","#         maxindex = int(np.argmax(emotion_prediction))\n","#         cv2.putText(frame, emotion_dict[maxindex], (x + 5, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n","\n","#     return frame"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T13:57:36.111500Z","iopub.status.busy":"2024-04-08T13:57:36.110765Z","iopub.status.idle":"2024-04-08T13:57:36.121810Z","shell.execute_reply":"2024-04-08T13:57:36.120601Z","shell.execute_reply.started":"2024-04-08T13:57:36.111464Z"},"trusted":true},"outputs":[],"source":["# # Source : https://www.youtube.com/watch?v=Lmmo71aZTLg\n","\n","# cap = cv2.VideoCapture('/kaggle/input/testing-image/Testing-sameple/emotion_sample_video.mp4')\n","\n","# ret, frame = cap.read()\n","\n","# frame_height, frame_width, _ = frame.shape\n","\n","# out = cv2.VideoWriter('newtry_normal_done.avi', cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), 10, (frame_width, frame_height))\n","# print(\"Processing Video......\")\n","\n","# while cap.isOpened():\n","#   ret, frame = cap.read()\n","#   if not ret:\n","#     out.release()\n","#     break\n","#   output = emotion_recog(frame)\n","#   out.write(output)\n","\n","# out.release()\n","# print(\"Done processing video\")"]},{"cell_type":"markdown","metadata":{},"source":["In the video demostration experiment, we can see that the green rectangle that using dlib frontal is more stable than the haar cascade. Therefore, dlib is better than haar cascade."]},{"cell_type":"markdown","metadata":{},"source":["Now, the problem is it only can detect the large faces. To able the model can detect the smaller faces, I will use another algorithm to combine with my own customized VGG16 model to perform better."]},{"cell_type":"markdown","metadata":{},"source":["### Frontal Faces + Small Faces"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T20:04:33.674402Z","iopub.status.busy":"2024-04-08T20:04:33.673898Z","iopub.status.idle":"2024-04-08T20:04:42.337481Z","shell.execute_reply":"2024-04-08T20:04:42.335586Z","shell.execute_reply.started":"2024-04-08T20:04:33.674365Z"},"trusted":true},"outputs":[],"source":["import dlib\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def predict_faces(face_rois):\n","    batch_size = len(face_rois)\n","    if batch_size == 0:\n","        return []\n","\n","    final_images = [cv2.resize(face_roi, (64, 64)) for face_roi in face_rois]\n","    final_images = np.stack(final_images)\n","    final_images = final_images / 255.0\n","\n","    predictions = final_vgg16_model.predict(final_images)\n","    # Process the predictions and return the results\n","    return predictions\n","\n","# Initialize the face detector from Dlib\n","face_detector = dlib.get_frontal_face_detector()\n","face_detector = dlib.cnn_face_detection_model_v1(\"/kaggle/input/fcae-detect/mmod_human_face_detector.dat\")\n","\n","# Load the video\n","video_path = '/kaggle/input/sad-cry/5981354-hd_1366_720_25fps.mp4'\n","cap = cv2.VideoCapture(video_path)\n","\n","# Get the video properties\n","fps = cap.get(cv2.CAP_PROP_FPS)\n","frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","# Define the codec and create a VideoWriter object\n","output_path = 'newtry_normal_done.avi'\n","fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")  # Adjust the codec as needed\n"," \n","output_video = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n","\n","# Dictionary to store emotion counts\n","emotion_counts = {\n","    \"Angry\": 0,\n","    \"Disgust\": 0,\n","    \"Fear\": 0,\n","    \"Happy\": 0,\n","    \"Neutral\": 0,\n","    \"Sad\": 0,\n","    \"Surprise\": 0\n","}\n","\n","# Dictionary to map numeric labels to emotion words\n","emotion_dict = {\n","    0: \"Angry\",\n","    1: \"Disgust\",\n","    2: \"Fear\",\n","    3: \"Happy\",\n","    4: \"Neutral\",\n","    5: \"Sad\",\n","    6: \"Surprise\"\n","}\n","\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    if not ret:\n","        output_video.release()\n","        break\n","    \n","    # Convert the frame to grayscale\n","    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","    \n","    # Detect faces in the frame\n","    faces = face_detector(gray)\n","    \n","    for face in faces:\n","        x, y, w, h = face.rect.left(), face.rect.top(), face.rect.width(), face.rect.height()\n","        \n","        # Draw a rectangle around the face\n","        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","        \n","        # Extract the face region\n","        face_region = frame[y:y+h, x:x+w]\n","        \n","        # Predict the face\n","        predictions = predict_faces([face_region])[0]\n","        \n","        # Find the index of the emotion with the highest predicted score\n","        max_index = np.argmax(predictions)\n","        \n","        # Get the emotion label with the highest predicted score\n","        emotion_label = emotion_dict[max_index]\n","        \n","        # Increment the count for the detected emotion\n","        emotion_counts[emotion_label] += 1\n","        \n","        # Display the emotion label\n","        cv2.putText(frame, emotion_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n","    \n","    # Display the emotion counts in the top right corner\n","    counts_text = \", \".join([f\"{emotion}: {count}\" for emotion, count in emotion_counts.items()])\n","    text_width, text_height = cv2.getTextSize(counts_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\n","    text_x = frame_width - text_width - 10\n","    text_y = 30\n","    \n","    cv2.putText(frame, counts_text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n","    \n","    # Write the annotated frame to the output video\n","    output_video.write(frame)\n","\n","cap.release()\n","output_video.release()\n","\n","print(\"Done processing video\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T13:54:52.685489Z","iopub.status.busy":"2024-04-11T13:54:52.684969Z","iopub.status.idle":"2024-04-11T16:08:26.554932Z","shell.execute_reply":"2024-04-11T16:08:26.553217Z","shell.execute_reply.started":"2024-04-11T13:54:52.685451Z"},"trusted":true},"outputs":[],"source":["def predict_faces(face_rois):\n","    batch_size = len(face_rois)\n","    if batch_size == 0:\n","        return []\n","\n","    final_images = [cv2.resize(face_roi, (64, 64)) for face_roi in face_rois]\n","    final_images = np.stack(final_images)\n","    final_images = final_images / 255.0\n","\n","    predictions = final_vgg16_model.predict(final_images)\n","    # Process the predictions and return the results\n","    return predictions\n","\n","# Initialize the face detector from Dlib\n","face_detector = dlib.get_frontal_face_detector()\n","face_detector = dlib.cnn_face_detection_model_v1(\"/kaggle/input/fcae-detect/mmod_human_face_detector.dat\")\n","\n","# Load the video\n","video_path = '/kaggle/input/distance-try/My Movie 5.mp4'\n","cap = cv2.VideoCapture(video_path)\n","\n","# Get the video properties\n","fps = cap.get(cv2.CAP_PROP_FPS)\n","frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","# Define the codec and create a VideoWriter object\n","output_path = 'newtry_normal_done.avi'\n","fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")  # Adjust the codec as needed\n"," \n","output_video = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n","\n","# Dictionary to store emotion counts\n","emotion_counts = {\n","    \"Angry\": 0,\n","    \"Disgust\": 0,\n","    \"Fear\": 0,\n","    \"Happy\": 0,\n","    \"Neutral\": 0,\n","    \"Sad\": 0,\n","    \"Surprise\": 0\n","}\n","\n","# Dictionary to map numeric labels to emotion words\n","emotion_dict = {\n","    0: \"Angry\",\n","    1: \"Disgust\",\n","    2: \"Fear\",\n","    3: \"Happy\",\n","    4: \"Neutral\",\n","    5: \"Sad\",\n","    6: \"Surprise\"\n","}\n","\n","# Set the threshold for alerting the teacher\n","alert_threshold = 1\n","\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    if not ret:\n","        output_video.release()\n","        break\n","    \n","    # Convert the frame to grayscale\n","    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","    \n","    # Detect faces in the frame\n","    faces = face_detector(gray)\n","    \n","    # Reset emotion counts for each frame\n","    emotion_counts = {\n","        \"Angry\": 0,\n","        \"Disgust\": 0,\n","        \"Fear\": 0,\n","        \"Happy\": 0,\n","        \"Neutral\": 0,\n","        \"Sad\": 0,\n","        \"Surprise\": 0\n","    }\n","    \n","    for face in faces:\n","        x, y, w, h = face.rect.left(), face.rect.top(), face.rect.width(), face.rect.height()\n","        \n","        # Draw a rectangle around the face\n","        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","        \n","        # Extract the face region\n","        face_region = frame[y:y+h, x:x+w]\n","        \n","        # Predict the face\n","        predictions = predict_faces([face_region])[0]\n","        \n","        # Find the index of the emotion with the highest predicted score\n","        max_index = np.argmax(predictions)\n","        \n","        # Get the emotion label with the highest predicted score\n","        emotion_label = emotion_dict[max_index]\n","        \n","        # Increment the count for the detected emotion\n","        emotion_counts[emotion_label] += 1\n","        \n","        # Display the emotion label\n","        cv2.putText(frame, emotion_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n","        \n","            # Check for high counts and display an alert if necessary\n","        if emotion_counts[\"Sad\"] > alert_threshold or emotion_counts[\"Disgust\"] > alert_threshold or emotion_counts[\"Fear\"] > alert_threshold or emotion_counts[\"Angry\"] > alert_threshold:\n","            alert_text = \"Alert: Teaching adjustment required!\"\n","            cv2.putText(frame, alert_text, (x, y - 40), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n","    \n","    # Display the emotion counts in the top right corner\n","    counts_text = \", \".join([f\"{emotion}: {count}\" for emotion, count in emotion_counts.items()])\n","    text_width, text_height = cv2.getTextSize(counts_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\n","    text_x = frame_width - text_width - 10\n","    text_y = 30\n","    \n","    cv2.putText(frame, counts_text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n","    \n","    # Write the annotated frame to the output video\n","    output_video.write(frame)\n","\n","cap.release()\n","output_video.release()\n","\n","print(\"Done processing video\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":2223442,"sourceId":3717583,"sourceType":"datasetVersion"},{"datasetId":4736976,"sourceId":8035646,"sourceType":"datasetVersion"},{"datasetId":4737180,"sourceId":8035924,"sourceType":"datasetVersion"},{"datasetId":4737234,"sourceId":8035984,"sourceType":"datasetVersion"},{"datasetId":4739294,"sourceId":8038787,"sourceType":"datasetVersion"},{"datasetId":4739482,"sourceId":8039062,"sourceType":"datasetVersion"},{"datasetId":4739580,"sourceId":8039203,"sourceType":"datasetVersion"},{"datasetId":4745853,"sourceId":8048093,"sourceType":"datasetVersion"},{"datasetId":4755899,"sourceId":8062244,"sourceType":"datasetVersion"},{"datasetId":4756180,"sourceId":8062620,"sourceType":"datasetVersion"},{"datasetId":4757361,"sourceId":8064152,"sourceType":"datasetVersion"},{"datasetId":4758187,"sourceId":8065272,"sourceType":"datasetVersion"},{"datasetId":4758239,"sourceId":8065365,"sourceType":"datasetVersion"},{"datasetId":4758998,"sourceId":8066365,"sourceType":"datasetVersion"},{"datasetId":4766001,"sourceId":8075946,"sourceType":"datasetVersion"},{"datasetId":4776981,"sourceId":8091206,"sourceType":"datasetVersion"},{"datasetId":4777364,"sourceId":8091751,"sourceType":"datasetVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"test_env1","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":4}
